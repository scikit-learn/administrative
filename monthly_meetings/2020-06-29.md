# June 29 2020 -- Scikit-learn dev meeting

### PRs needing decision

#### From last meeting
- (Nicolas) Parameter about handling unkown values in `OrdinalEncoder`: see [discussion](https://github.com/scikit-learn/scikit-learn/pull/16959#discussion_r422499148).
Olivier argues for specifying replacement value for unknowns in the input category space; downside may be difficulty of expressing "category below lowest".
A simplified version of this PR has been opened by @FelixWick [#17406](https://github.com/scikit-learn/scikit-learn/pull/17406).
Please comment in the new PR.

### Other topics
- (Roman) code formatting: either fully fix flake8 or use black [#11336](https://github.com/scikit-learn/scikit-learn/issues/11336).
Also a reminder that linters (including mypy) can be run for each commit locally with [pre-commit](https://pre-commit.com/).
Black will work better with longer lines and we will be forced to follow it.
  - Adrin: Changes in black will lead to large diffs in the repo.
  - Gael: Why make a standard that doesn't follow the standard? (pep8)
  - Joel/Adrin: Enforcing flake8 makes it unnecessary hard for new contributors who need to manually fix obscure flake8 errors.
  - Gael summary: Everybody is OK going either way.
- (Andy) coming back to a minimal implementation of feature names: [#12627](https://github.com/scikit-learn/scikit-learn/pull/12627) given that we seem
stuck on sparse support. Should I write a SLEP? _I missed the answer here, if there was one..._
- (Andy) Resampling [SLEP 5](https://github.com/scikit-learn/enhancement_proposals/pull/15): what should next steps be?
    - We all agree that `y` shouldn't be changed in the test set
    - This is related to **breaking `fit_transform() == fit().transform()`, which we should consider seriously**
      - Examples of when this is useful: TargetEncoderCV, Stacking
    - Joel: breaking the fit_transform equivalence is unrelated to the resampling.
        - could also introduce a new verb instead of `fit`, and have it `train`, but this breaks too many things.
    - Joel proposed to implement resamplers as a meta-estimator (ResampledTrainer) to simplify implementation and semantics
- (Thomas) array_out configuration (pandas or xarray or pydata/sparse are supported) [#16772](https://github.com/scikit-learn/scikit-learn/pull/16772)
    - dense cases are fine (no computational overhead) independent of the output type
    - on sparse cases: pydata/sparse and xarray use more
- (Joel) sample props [SLEP006](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep006/proposal.html) is progressing nicely.
We seem to be going with "[solution 4](https://github.com/scikit-learn/scikit-learn/pull/16079)", although there are some implementation details that need
ironing out in it and some questions about backwards compatibility. I'd like your thoughts on naming: do we continue to call these "sample props"?
Is there a better name?
    - proposal modifies clone
    - naming: use `request_metadata` instead of `properties` (preferable to `props`)
    - supporting pandas and xarray as in / out will open the door to future evolution, and then we can deprecate supporting non-sample aligned data
    - question of supporting metadata not aligned to samples: let's allow whatever, but maybe take away ability to pass not-aligned metadata in the future?

### New PRs
- (Jeremie) ASV benchmarks [#17026](https://github.com/scikit-learn/scikit-learn/pull/17026) could not find any perf regression between 0.22 and 0.23.
Is there a way and will to integrate this further in the scikit-learn organization? Yes with the following steps:
    - Update the [benchmarks repo](https://github.com/scikit-learn/scikit-learn-speed) on sklearn repository.
    - Merge the benchmark PR [#17026](https://github.com/scikit-learn/scikit-learn/pull/17026) in sklearn 
    - Results to be published gh-pages at: https://github.com/scikit-learn/scikit-learn-speed

### General topics
Feedback on development sprints.
  - Many doc improvements merged and not too many new unmerged PRs (I think - olivier)
  - (Olivier) More prototyping on NEP-18/NEP-35 vs NEP-37 to accept CuPy and other numpy compatible arrays in PCA & preprocessing transformers:
    - NEP-18 for scalers tested with CuPy [#17744](https://github.com/scikit-learn/scikit-learn/pull/17744)
    - NEP-37 randomized svd tested with CuPy [#17676](https://github.com/scikit-learn/scikit-learn/pull/17676) (NEP-18 is not enough b/c we need sampling)
        - A blocker is continuous integration for GPU (maybe Azure pipelines can sponsor us here: we would need a GPU-enabled worker if that exists)
        - Andy mentions that mxnet has a fairly complete numpy compatibility
  - (Olivier) Progress on classifier calibration [#11096](https://github.com/scikit-learn/scikit-learn/pull/11096) [#17443](https://github.com/scikit-learn/scikit-learn/pull/17443) [#16321](https://github.com/scikit-learn/scikit-learn/issues/16321)

During sprints, please always sync with upstream before merge to check that things are not broken (too many concurrent PRs).
To prevent CI failures (that happen repeatedly recently):
- Run cron jobs weekly, in order to check consistency more often.
- Create a key in order to trigger CI with specific commits.

Next sprints are listed [here](https://github.com/scikit-learn/scikit-learn/wiki/Upcoming-events)
