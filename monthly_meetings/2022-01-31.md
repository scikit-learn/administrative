# January 31 2022 @ 20:00 UTC

When you add an entry, please add you  name next to it.

### Topics
 - (Adrin): SLEP006, vote and implementation
     - Passing around metadata is a problem for users and developers that we've been trying to solve for a long time, and we should seriously consider trying to conclude it soon.
     - SLEP is about the user API, the implementation is still open to change
     - implementation tries to make it easier for developers to use which includes a little more "Python magic" than scikit-learn traditionally has: BaseEstimator will do a lot of the work for non-metaestimators
     - **Action item**: Ask `cuml` about metadata routing
     - https://github.com/scikit-learn/enhancement_proposals/pull/65
     - Rendered SLEP: https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep006/proposal.html
     - Idea: provide base package without any concrete scikit-learn implementations but would provide base classes / utils?
         - To be done before merging in main branch, but not necessary to move forward
 - (Christian?): Discuss roadmap update?
     - Everybody: write down a few bullet points/thoughts (keep it not too long), and copy in the hack MD below
     - https://hackmd.io/EnU2zmM-Q0OWDmW4Y9UoXQ?both
     - Later consolidate by merging common ideas and then discuss around this common proposal.

### Need decision
 - (Thomas): (Christian's PR) and Quantile Hist Gradient Boosting API [#21800](https://github.com/scikit-learn/scikit-learn/pull/21800)
   `HistGradientBoostingRegressor(loss="quantile", loss_param=??)`
   Do we anticipate losses with 2 parameters?
   Proposed solutions:
    1. If we want to be fully generic, `loss_param` could become a dictionary, which will be passed down into the loss directly. This way, other parameterized losses can be generically supported by this dictionary. We kind of already do this with metric_kwargs.
    2. We can copy GradientBoostingRegressor's API and use a single parameter (`alpha`) that is just for the quantile loss. In HistGradientBoostingRegressor case, I would call it `quantile` instead of `alpha`.
    3. (Likely out of reach in the near-term) Loss become public and users can pass create a `PinballLoss(quantile=0.3)` and pass it directly into HistGradient*.
    4. `loss_param` is single parameter with different meaning depending on the loss, i.e. a quantile level for `loss="quantile"`, and tweedie power for `loss="tweedie"` (in a later PR).
    5. (Olivier) allow for:
        ```python
        HistGradientBoostingRegressor(
            loss="squared_error"
        )
        HistGradientBoostingRegressor(
            loss={"name": "quantile", "quantile": 0.9}
        )
        HistGradientBoostingRegressor(
            loss=LossClass(loss_param=value),  # maybe in the future
        )

        # Current PR's API
        HistGradientBoostingRegressor(
            loss="quantile", loss_param=0.3
        )
        # (Joel in chat)
        HistGradientBoostingRegressor(
            loss="quantile:0.3"
        )
        ```
        Pros: avoid implicit interactions between several hyper-params
        Cons:
            - makes it harder/impossible to grid-search such nested parameters but maybe there is no point in doing so.
            - dicts prevent discovery via auto-complete

        Could be generalized to other similar code patterns:
            - kernels for SVMs / GP
            - metrics?
 - (Thomas): array-api + scikit-learn demo (If time permits)

### Need attention (reviews)

### Communications

* (Lauren): blog
    * Suggestions and discussion about blog design: https://github.com/scikit-learn/blog/issues/67
    * If we want to make changes, we need to decide now which theme we are going to build upon
        * If we want to make intensive changes, we need to pick the theme now

### For information

 - (Olivier): Computational routines extension points
     - (Gael): partnering with scikit-learn to develop optimized kernels
 - (Fran√ßois): 2 upcoming sprints (Mercado Libre + WIDSML)

### Next meeting date and chair person
- Feb 28 (20UTC) Thomas
